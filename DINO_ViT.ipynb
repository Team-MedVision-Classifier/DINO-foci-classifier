{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-i20ctUXoSv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm pandas --quiet"
      ],
      "metadata": {
        "id": "QdZJUfyqXzaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import timm\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "7kcr1JPZX1YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DINO 2"
      ],
      "metadata": {
        "id": "_NWsqYy0YCWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import timm\n",
        "\n",
        "# -----------------------\n",
        "# Config\n",
        "# -----------------------\n",
        "IMAGE_SIZE = 224\n",
        "LABEL_COL = \"label\"\n",
        "CSV_PATH = \"/content/drive/MyDrive/FYP/foci_labels_v02.csv\"\n",
        "\n",
        "# EXACT supervised split sizes\n",
        "SUP_SPLIT = {\n",
        "    0: {\"train\": 730,  \"val\": 50,  \"test\": 41},\n",
        "    1: {\"train\": 2000, \"val\": 100, \"test\": 62},\n",
        "}\n",
        "\n",
        "# SSL + eval settings\n",
        "BATCH_SSL = 64\n",
        "BATCH_EVAL = 64\n",
        "NUM_EPOCHS = 100\n",
        "EVAL_EVERY = 10          # run kNN eval every N epochs\n",
        "KNN_K = 20               # k in kNN\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -----------------------\n",
        "# Datasets\n",
        "# -----------------------\n",
        "class DINOUnlabeledDataset(Dataset):\n",
        "    def __init__(self, df, transform_global):\n",
        "        self.paths = df[\"image_path\"].tolist()\n",
        "        self.transform_global = transform_global\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        v1 = self.transform_global(img)\n",
        "        v2 = self.transform_global(img)\n",
        "        return v1, v2\n",
        "\n",
        "class LabeledImageDataset(Dataset):\n",
        "    def __init__(self, df, transform):\n",
        "        self.paths = df[\"image_path\"].tolist()\n",
        "        self.labels = df[LABEL_COL].astype(int).tolist()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        x = self.transform(img)\n",
        "        y = self.labels[idx]\n",
        "        return x, y\n",
        "\n",
        "# -----------------------\n",
        "# Transforms\n",
        "# -----------------------\n",
        "global_transform = T.Compose([\n",
        "    # T.RandomResizedCrop(IMAGE_SIZE, scale=(0.6, 1.0)),\n",
        "    T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomVerticalFlip(),\n",
        "    T.RandomRotation(degrees=20),\n",
        "    # T.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.25, 0.25, 0.25]),\n",
        "])\n",
        "\n",
        "eval_transform = T.Compose([\n",
        "    T.Resize(256),\n",
        "    T.CenterCrop(IMAGE_SIZE),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.25, 0.25, 0.25]),\n",
        "])\n",
        "\n",
        "\n",
        "df_full = pd.read_csv(CSV_PATH)\n",
        "df_full = df_full.drop_duplicates(subset=[\"image_path\"]).reset_index(drop=True)\n",
        "\n",
        "df_class0 = df_full[df_full[LABEL_COL] == 0].copy()\n",
        "df_class1 = df_full[df_full[LABEL_COL] == 1].copy()\n",
        "\n",
        "print(\"Total images:\", len(df_full))\n",
        "print(\"Label 0 count:\", len(df_class0))\n",
        "print(\"Label 1 count:\", len(df_class1))\n",
        "\n",
        "for lbl, req in SUP_SPLIT.items():\n",
        "    have = len(df_full[df_full[LABEL_COL] == lbl])\n",
        "    need = req[\"train\"] + req[\"val\"] + req[\"test\"]\n",
        "    assert have >= need, f\"Not enough samples for label {lbl}: have {have}, need {need}\"\n",
        "\n",
        "# -----------------------\n",
        "# Build EXACT supervised splits (train/val/test)\n",
        "# -----------------------\n",
        "def split_exact(df_lbl: pd.DataFrame, n_train: int, n_val: int, n_test: int, seed: int = 42):\n",
        "    df_test = df_lbl.sample(n=n_test, random_state=seed)\n",
        "    df_rem = df_lbl.drop(df_test.index)\n",
        "    df_val = df_rem.sample(n=n_val, random_state=seed)\n",
        "    df_train = df_rem.drop(df_val.index)\n",
        "    assert len(df_train) == n_train, f\"Train size mismatch: got {len(df_train)} expected {n_train}\"\n",
        "    return df_train, df_val, df_test\n",
        "\n",
        "df0_train, df0_val, df0_test = split_exact(\n",
        "    df_class0, SUP_SPLIT[0][\"train\"], SUP_SPLIT[0][\"val\"], SUP_SPLIT[0][\"test\"], seed=42\n",
        ")\n",
        "df1_train, df1_val, df1_test = split_exact(\n",
        "    df_class1, SUP_SPLIT[1][\"train\"], SUP_SPLIT[1][\"val\"], SUP_SPLIT[1][\"test\"], seed=42\n",
        ")\n",
        "\n",
        "df_train = pd.concat([df0_train, df1_train]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "df_val   = pd.concat([df0_val,   df1_val]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "df_test  = pd.concat([df0_test,  df1_test]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nSupervised splits:\")\n",
        "print(\"Train:\", len(df_train), \" (label0:\", (df_train[LABEL_COL]==0).sum(), \"label1:\", (df_train[LABEL_COL]==1).sum(), \")\")\n",
        "print(\"Val:  \", len(df_val),   \" (label0:\", (df_val[LABEL_COL]==0).sum(),   \"label1:\", (df_val[LABEL_COL]==1).sum(),   \")\")\n",
        "print(\"Test: \", len(df_test),  \" (label0:\", (df_test[LABEL_COL]==0).sum(),  \"label1:\", (df_test[LABEL_COL]==1).sum(),  \")\")\n",
        "\n",
        "train_csv = \"/content/drive/MyDrive/FYP/foci_supervised_train_v02.csv\"\n",
        "val_csv   = \"/content/drive/MyDrive/FYP/foci_supervised_val_v02.csv\"\n",
        "test_csv  = \"/content/drive/MyDrive/FYP/foci_supervised_test_v02.csv\"\n",
        "df_train.to_csv(train_csv, index=False)\n",
        "df_val.to_csv(val_csv, index=False)\n",
        "df_test.to_csv(test_csv, index=False)\n",
        "\n",
        "print(\"\\nSaved:\")\n",
        "print(\" -\", train_csv)\n",
        "print(\" -\", val_csv)\n",
        "print(\" -\", test_csv)\n",
        "\n",
        "# -----------------------\n",
        "# SSL dataset: use ALL images\n",
        "# -----------------------\n",
        "df_unlabeled = df_full[[\"image_path\"]].reset_index(drop=True)\n",
        "\n",
        "unlab_dataset = DINOUnlabeledDataset(df_unlabeled, transform_global=global_transform)\n",
        "unlab_loader = DataLoader(\n",
        "    unlab_dataset,\n",
        "    batch_size=BATCH_SSL,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    drop_last=True\n",
        ")\n",
        "print(\"Unlabeled images used for SSL (ALL):\", len(unlab_dataset))\n",
        "\n",
        "# -----------------------\n",
        "# DINO backbone + head\n",
        "# -----------------------\n",
        "class ViTBackbone(nn.Module):\n",
        "    def __init__(self, name=\"vit_tiny_patch16_224\", pretrained=True):\n",
        "        super().__init__()\n",
        "        self.vit = timm.create_model(\n",
        "            name,\n",
        "            pretrained=pretrained,\n",
        "            num_classes=0,\n",
        "            global_pool=\"\"\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.vit.forward_features(x)\n",
        "        if isinstance(feats, dict):\n",
        "            if \"x\" in feats:\n",
        "                feats = feats[\"x\"]\n",
        "            elif \"pooled\" in feats:\n",
        "                feats = feats[\"pooled\"]\n",
        "        if feats.dim() == 3:\n",
        "            feats = feats[:, 0, :]\n",
        "        return feats  # (B, D)\n",
        "\n",
        "class DINOHead(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim=256, hidden_dim=2048, bottleneck_dim=256):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, bottleneck_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(bottleneck_dim, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "class DINOStudentTeacher(nn.Module):\n",
        "    def __init__(self,\n",
        "                 backbone_name=\"vit_tiny_patch16_224\",\n",
        "                 backbone_pretrained=True,\n",
        "                 out_dim=256,\n",
        "                 hidden_dim=2048,\n",
        "                 bottleneck_dim=256):\n",
        "        super().__init__()\n",
        "        self.backbone = ViTBackbone(backbone_name, pretrained=backbone_pretrained)\n",
        "\n",
        "        dummy = torch.zeros(1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
        "        with torch.no_grad():\n",
        "            in_dim = self.backbone(dummy).shape[-1]\n",
        "\n",
        "        self.head = DINOHead(in_dim, out_dim, hidden_dim, bottleneck_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.backbone(x)\n",
        "        out = self.head(feats)\n",
        "        return out\n",
        "\n",
        "# -----------------------\n",
        "# DINO loss\n",
        "# -----------------------\n",
        "@dataclass\n",
        "class DINOLossConfig:\n",
        "    out_dim: int = 256\n",
        "    teacher_temp: float = 0.04\n",
        "    student_temp: float = 0.1\n",
        "    center_momentum: float = 0.9\n",
        "\n",
        "class DINOLoss(nn.Module):\n",
        "    def __init__(self, cfg: DINOLossConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.register_buffer(\"center\", torch.zeros(1, cfg.out_dim))\n",
        "\n",
        "    def forward(self, student_outputs, teacher_outputs):\n",
        "        student_temp = self.cfg.student_temp\n",
        "        teacher_temp = self.cfg.teacher_temp\n",
        "\n",
        "        student_out = [F.softmax(s / student_temp, dim=-1) for s in student_outputs]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            teacher_out = []\n",
        "            for t in teacher_outputs:\n",
        "                t = (t - self.center)\n",
        "                t = F.softmax(t / teacher_temp, dim=-1)\n",
        "                teacher_out.append(t)\n",
        "\n",
        "        total_loss = 0.0\n",
        "        n_terms = 0\n",
        "        for i, t_out in enumerate(teacher_out):\n",
        "            for j, s_out in enumerate(student_out):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                loss = torch.sum(-t_out * torch.log(s_out + 1e-6), dim=-1)\n",
        "                total_loss += loss.mean()\n",
        "                n_terms += 1\n",
        "\n",
        "        total_loss /= max(1, n_terms)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            batch_center = torch.cat(teacher_out, dim=0).mean(dim=0, keepdim=True)\n",
        "            self.center = self.center * self.cfg.center_momentum + batch_center * (1.0 - self.cfg.center_momentum)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "# -----------------------\n",
        "# Build student/teacher, optimizer\n",
        "# -----------------------\n",
        "OUT_DIM = 256\n",
        "student = DINOStudentTeacher(\"vit_tiny_patch16_224\", True, OUT_DIM).to(device)\n",
        "teacher = DINOStudentTeacher(\"vit_tiny_patch16_224\", True, OUT_DIM).to(device)\n",
        "\n",
        "teacher.load_state_dict(student.state_dict())\n",
        "for p in teacher.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "dino_loss_fn = DINOLoss(DINOLossConfig(out_dim=OUT_DIM)).to(device)\n",
        "optimizer = torch.optim.AdamW(student.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "teacher_momentum = 0.996\n",
        "\n",
        "@torch.no_grad()\n",
        "def update_teacher(student, teacher, m):\n",
        "    for ps, pt in zip(student.parameters(), teacher.parameters()):\n",
        "        pt.data.mul_(m).add_(ps.data, alpha=(1.0 - m))\n",
        "\n",
        "# -----------------------\n",
        "# kNN probe (teacher backbone) for val monitoring\n",
        "# -----------------------\n",
        "@torch.no_grad()\n",
        "def extract_features(backbone: nn.Module, loader: DataLoader):\n",
        "    backbone.eval()\n",
        "    feats_all, labels_all = [], []\n",
        "    for x, y in tqdm(loader, desc=\"Extract feats\", leave=False):\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        f = backbone(x)                 # (B, D)\n",
        "        f = F.normalize(f, dim=-1)      # cosine space\n",
        "        feats_all.append(f.cpu())\n",
        "        labels_all.append(y.cpu())\n",
        "    return torch.cat(feats_all, dim=0), torch.cat(labels_all, dim=0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def knn_accuracy(train_feats, train_labels, val_feats, val_labels, k=20):\n",
        "    sims = val_feats @ train_feats.T\n",
        "    topk = sims.topk(k=k, dim=1).indices\n",
        "    topk_labels = train_labels[topk]\n",
        "\n",
        "    # majority vote\n",
        "    preds = []\n",
        "    for row in topk_labels:\n",
        "        counts = torch.bincount(row, minlength=int(train_labels.max().item()) + 1)\n",
        "        preds.append(counts.argmax().item())\n",
        "    preds = torch.tensor(preds)\n",
        "\n",
        "    acc = (preds == val_labels).float().mean().item()\n",
        "    return acc\n",
        "\n",
        "train_eval_ds = LabeledImageDataset(df_train, transform=eval_transform)\n",
        "val_eval_ds   = LabeledImageDataset(df_val,   transform=eval_transform)\n",
        "\n",
        "train_eval_loader = DataLoader(train_eval_ds, batch_size=BATCH_EVAL, shuffle=False, num_workers=2, pin_memory=True)\n",
        "val_eval_loader   = DataLoader(val_eval_ds,   batch_size=BATCH_EVAL, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# -----------------------\n",
        "# SSL training loop + periodic kNN eval\n",
        "# -----------------------\n",
        "ssl_loss_history = []\n",
        "knn_val_history = []\n",
        "\n",
        "student.train()\n",
        "teacher.eval()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for v1, v2 in tqdm(unlab_loader, desc=f\"SSL Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
        "        v1 = v1.to(device, non_blocking=True)\n",
        "        v2 = v2.to(device, non_blocking=True)\n",
        "\n",
        "        s1 = student(v1)\n",
        "        s2 = student(v2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            t1 = teacher(v1)\n",
        "            t2 = teacher(v2)\n",
        "\n",
        "        loss = dino_loss_fn([s1, s2], [t1, t2])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        update_teacher(student, teacher, teacher_momentum)\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    avg_loss = epoch_loss / max(1, n_batches)\n",
        "    ssl_loss_history.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - SSL DINO Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # ---- periodic kNN monitoring on supervised VAL ----\n",
        "    if (epoch + 1) % EVAL_EVERY == 0 or (epoch == 0):\n",
        "        print(f\"\\n[kNN probe] Epoch {epoch+1}: extracting features from teacher backbone...\")\n",
        "        tr_f, tr_y = extract_features(teacher.backbone, train_eval_loader)\n",
        "        va_f, va_y = extract_features(teacher.backbone, val_eval_loader)\n",
        "        acc = knn_accuracy(tr_f, tr_y, va_f, va_y, k=KNN_K)\n",
        "        knn_val_history.append((epoch + 1, acc))\n",
        "        print(f\"[kNN probe] VAL accuracy @k={KNN_K}: {acc*100:.2f}%\\n\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(1, len(ssl_loss_history) + 1), ssl_loss_history, marker=\"o\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"SSL DINO Loss\")\n",
        "plt.title(\"DINO SSL Training Loss (ALL images)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "if len(knn_val_history) > 0:\n",
        "    xs = [e for e, _ in knn_val_history]\n",
        "    ys = [a for _, a in knn_val_history]\n",
        "    plt.figure()\n",
        "    plt.plot(xs, ys, marker=\"o\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"kNN Val Accuracy\")\n",
        "    plt.title(f\"kNN Probe on Val (k={KNN_K}) using Teacher Backbone Features\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------\n",
        "# Save only backbone from EMA teacher\n",
        "# -----------------------\n",
        "ssl_ckpt_path = \"/content/drive/MyDrive/FYP/foci_dino_backbone_ALLimgs_v03.pth\"\n",
        "torch.save(teacher.backbone.state_dict(), ssl_ckpt_path)\n",
        "print(\"\\nSaved DINO-pretrained backbone to:\", ssl_ckpt_path)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2_nzCsb7X9ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import timm\n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "LABEL_COL = \"label\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Path to DINO backbone (EMA teacher backbone)\n",
        "dino_backbone_ckpt = \"/content/drive/MyDrive/FYP/foci_dino_backbone_ALLimgs_v03.pth\"\n",
        "# (set this to whatever you actually saved)\n",
        "\n",
        "heldout_csv_path = \"/content/drive/MyDrive/FYP/foci_heldout_for_supervised.csv\"\n",
        "\n",
        "supervised_ckpt_path = \"/content/drive/MyDrive/FYP/foci_dino_supervised_classifier_v03.pth\"\n",
        "\n",
        "# ---- Transforms ----\n",
        "train_transform = T.Compose([\n",
        "    # T.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
        "    T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomVerticalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                std=[0.25, 0.25, 0.25]),\n",
        "])\n",
        "\n",
        "eval_transform = T.Compose([\n",
        "    T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                std=[0.25, 0.25, 0.25]),\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "k8SVVu_HaeSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Dataset for labeled classification\n",
        "# -------------------------------\n",
        "class FociLabeledDataset(Dataset):\n",
        "    def __init__(self, df, transform):\n",
        "        self.paths = df[\"image_path\"].tolist()\n",
        "        self.labels = df[LABEL_COL].astype(float).tolist()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.paths[idx]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return img, label\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# ViT backbone (same as SSL)\n",
        "# -------------------------------\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "\n",
        "class ViTBackbone(nn.Module):\n",
        "    def __init__(self, name=\"vit_tiny_patch16_224\", pretrained=False):\n",
        "        super().__init__()\n",
        "        self.vit = timm.create_model(\n",
        "            name,\n",
        "            # Ypretrained=False because later load DINO-pretrained weights instead of ImageNet weights.\n",
        "            pretrained=pretrained,   # for DINO backbone, set False; weights come from ckpt\n",
        "            num_classes=0,\n",
        "            global_pool=\"\"\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Return a single feature vector per image: (B, D)\n",
        "        by taking the CLS token (token at index 0).\n",
        "        \"\"\"\n",
        "        feats = self.vit.forward_features(x)  # tensor or dict\n",
        "\n",
        "\n",
        "        # B = batch size,\n",
        "        # N = number of tokens (1 CLS + patch tokens),\n",
        "        # D = embedding dimension\n",
        "\n",
        "\n",
        "        if isinstance(feats, dict):\n",
        "            if \"x\" in feats:\n",
        "                feats = feats[\"x\"]      # (B, N, D)\n",
        "            elif \"pooled\" in feats:\n",
        "                feats = feats[\"pooled\"] # (B, D)\n",
        "\n",
        "        if feats.dim() == 3:\n",
        "            # CLS token\n",
        "            feats = feats[:, 0, :]      # (B, D)\n",
        "\n",
        "        return feats                    # (B, D)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Classifier using DINO backbone\n",
        "# -------------------------------\n",
        "class FociDINOClassifier(nn.Module):\n",
        "    def __init__(self, backbone_ckpt, backbone_name=\"vit_tiny_patch16_224\"):\n",
        "        super().__init__()\n",
        "        # use CLS-pooling backbone\n",
        "        self.backbone = ViTBackbone(backbone_name, pretrained=False)\n",
        "\n",
        "        # load DINO backbone weights\n",
        "        state = torch.load(backbone_ckpt, map_location=\"cpu\")\n",
        "        self.backbone.load_state_dict(state)\n",
        "\n",
        "        # freeze backbone for linear probe\n",
        "        for p in self.backbone.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # infer embedding dim\n",
        "        dummy = torch.zeros(1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
        "        with torch.no_grad():\n",
        "            in_dim = self.backbone(dummy).shape[-1]   # now (1, D)\n",
        "\n",
        "        # Classification head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(in_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 1)  # binary logit\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.backbone(x)           # (B, D)\n",
        "        logit = self.head(feats).squeeze(-1)  # (B,)\n",
        "        return logit"
      ],
      "metadata": {
        "id": "ObS4-mWTard_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import timm\n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "LABEL_COL = \"label\"\n",
        "CSV_PATH = \"/content/drive/MyDrive/FYP/foci_labels_v02.csv\"\n",
        "\n",
        "# EXACT supervised split sizes\n",
        "SUP_SPLIT = {\n",
        "    0: {\"train\": 730,  \"val\": 50,  \"test\": 41},\n",
        "    1: {\"train\": 2000, \"val\": 100, \"test\": 62},\n",
        "}\n",
        "\n",
        "# SSL + eval settings\n",
        "BATCH_SSL = 64\n",
        "BATCH_EVAL = 64\n",
        "NUM_EPOCHS = 100\n",
        "EVAL_EVERY = 10          # run kNN eval every N epochs\n",
        "KNN_K = 20               # k in kNN\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -----------------------\n",
        "# Datasets\n",
        "# -----------------------\n",
        "class DINOUnlabeledDataset(Dataset):\n",
        "    def __init__(self, df, transform_global):\n",
        "        self.paths = df[\"image_path\"].tolist()\n",
        "        self.transform_global = transform_global\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        v1 = self.transform_global(img)\n",
        "        v2 = self.transform_global(img)\n",
        "        return v1, v2\n",
        "\n",
        "class LabeledImageDataset(Dataset):\n",
        "    def __init__(self, df, transform):\n",
        "        self.paths = df[\"image_path\"].tolist()\n",
        "        self.labels = df[LABEL_COL].astype(int).tolist()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        x = self.transform(img)\n",
        "        y = self.labels[idx]\n",
        "        return x, y\n",
        "\n",
        "# -----------------------\n",
        "# Transforms\n",
        "# -----------------------\n",
        "global_transform = T.Compose([\n",
        "    # T.RandomResizedCrop(IMAGE_SIZE, scale=(0.6, 1.0)),\n",
        "    T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomVerticalFlip(),\n",
        "    T.RandomRotation(degrees=20),\n",
        "    # T.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.25, 0.25, 0.25]),\n",
        "])\n",
        "\n",
        "# Deterministic transform for feature extraction / eval\n",
        "eval_transform = T.Compose([\n",
        "    T.Resize(256),\n",
        "    T.CenterCrop(224),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.25, 0.25, 0.25]),\n",
        "])\n",
        "\n",
        "\n",
        "# global_transform = T.Compose([\n",
        "#     # T.RandomResizedCrop(IMAGE_SIZE, scale=(0.6, 1.0)),\n",
        "#     T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "#     T.RandomHorizontalFlip(),\n",
        "#     T.RandomVerticalFlip(),\n",
        "#     T.RandomRotation(degrees=20),\n",
        "#     # T.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "#     T.ToTensor(),\n",
        "#     T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.25, 0.25, 0.25]),\n",
        "# ])\n",
        "\n",
        "# # Deterministic transform for feature extraction / eval\n",
        "# eval_transform = T.Compose([\n",
        "#     T.Resize(256),\n",
        "#     T.CenterCrop(IMAGE_SIZE),\n",
        "#     T.ToTensor(),\n",
        "#     T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.25, 0.25, 0.25]),\n",
        "# ])\n",
        "\n",
        "\n",
        "df_full = pd.read_csv(CSV_PATH)\n",
        "df_full = df_full.drop_duplicates(subset=[\"image_path\"]).reset_index(drop=True)\n",
        "\n",
        "df_class0 = df_full[df_full[LABEL_COL] == 0].copy()\n",
        "df_class1 = df_full[df_full[LABEL_COL] == 1].copy()\n",
        "\n",
        "print(\"Total images:\", len(df_full))\n",
        "print(\"Label 0 count:\", len(df_class0))\n",
        "print(\"Label 1 count:\", len(df_class1))\n",
        "\n",
        "for lbl, req in SUP_SPLIT.items():\n",
        "    have = len(df_full[df_full[LABEL_COL] == lbl])\n",
        "    need = req[\"train\"] + req[\"val\"] + req[\"test\"]\n",
        "    assert have >= need, f\"Not enough samples for label {lbl}: have {have}, need {need}\"\n",
        "\n",
        "# -----------------------\n",
        "# Build EXACT supervised splits (train/val/test)\n",
        "# -----------------------\n",
        "def split_exact(df_lbl: pd.DataFrame, n_train: int, n_val: int, n_test: int, seed: int = 42):\n",
        "    df_test = df_lbl.sample(n=n_test, random_state=seed)\n",
        "    df_rem = df_lbl.drop(df_test.index)\n",
        "    df_val = df_rem.sample(n=n_val, random_state=seed)\n",
        "    df_train = df_rem.drop(df_val.index)\n",
        "    assert len(df_train) == n_train, f\"Train size mismatch: got {len(df_train)} expected {n_train}\"\n",
        "    return df_train, df_val, df_test\n",
        "\n",
        "df0_train, df0_val, df0_test = split_exact(\n",
        "    df_class0, SUP_SPLIT[0][\"train\"], SUP_SPLIT[0][\"val\"], SUP_SPLIT[0][\"test\"], seed=42\n",
        ")\n",
        "df1_train, df1_val, df1_test = split_exact(\n",
        "    df_class1, SUP_SPLIT[1][\"train\"], SUP_SPLIT[1][\"val\"], SUP_SPLIT[1][\"test\"], seed=42\n",
        ")\n",
        "\n",
        "df_train = pd.concat([df0_train, df1_train]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "df_val   = pd.concat([df0_val,   df1_val]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "df_test  = pd.concat([df0_test,  df1_test]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nSupervised splits:\")\n",
        "print(\"Train:\", len(df_train), \" (label0:\", (df_train[LABEL_COL]==0).sum(), \"label1:\", (df_train[LABEL_COL]==1).sum(), \")\")\n",
        "print(\"Val:  \", len(df_val),   \" (label0:\", (df_val[LABEL_COL]==0).sum(),   \"label1:\", (df_val[LABEL_COL]==1).sum(),   \")\")\n",
        "print(\"Test: \", len(df_test),  \" (label0:\", (df_test[LABEL_COL]==0).sum(),  \"label1:\", (df_test[LABEL_COL]==1).sum(),  \")\")\n",
        "\n",
        "train_csv = \"/content/drive/MyDrive/FYP/foci_supervised_train_v02.csv\"\n",
        "val_csv   = \"/content/drive/MyDrive/FYP/foci_supervised_val_v02.csv\"\n",
        "test_csv  = \"/content/drive/MyDrive/FYP/foci_supervised_test_v02.csv\"\n",
        "df_train.to_csv(train_csv, index=False)\n",
        "df_val.to_csv(val_csv, index=False)\n",
        "df_test.to_csv(test_csv, index=False)\n",
        "\n",
        "print(\"\\nSaved:\")\n",
        "print(\" -\", train_csv)\n",
        "print(\" -\", val_csv)\n",
        "print(\" -\", test_csv)\n",
        "\n",
        "print(\"Train size:\", len(df_train))\n",
        "print(\"Val size:\", len(df_val))\n",
        "print(\"Test size:\", len(df_test))\n",
        "\n",
        "train_dataset = FociLabeledDataset(df_train, transform=train_transform)\n",
        "val_dataset   = FociLabeledDataset(df_val, transform=eval_transform)\n",
        "test_dataset  = FociLabeledDataset(df_test, transform=eval_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "dN5Kaoe_atCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train / eval helper functions"
      ],
      "metadata": {
        "id": "SHvLjhoHaxKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for imgs, labels in tqdm(loader, desc=\"Train\", leave=False):\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        logits = model(imgs)  # (B,)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = (probs >= 0.5).float()\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    avg_loss = epoch_loss / max(1, total)\n",
        "    acc = correct / max(1, total)\n",
        "    return avg_loss, acc\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, prefix=\"Val\"):\n",
        "    model.eval()\n",
        "    epoch_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for imgs, labels in tqdm(loader, desc=prefix, leave=False):\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        epoch_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = (probs >= 0.5).float()\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    avg_loss = epoch_loss / max(1, total)\n",
        "    acc = correct / max(1, total)\n",
        "    return avg_loss, acc\n"
      ],
      "metadata": {
        "id": "ELN76k_Gaxen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supervised training loop + plots"
      ],
      "metadata": {
        "id": "OurcMGaTa0oc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dino_backbone_ckpt"
      ],
      "metadata": {
        "id": "EHwZMmnZsDrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "supervised_ckpt_path"
      ],
      "metadata": {
        "id": "3NmTM2QIsGfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Supervised training loop (with val)\n",
        "# -------------------------------\n",
        "model = FociDINOClassifier(backbone_ckpt=dino_backbone_ckpt).to(device)\n",
        "\n",
        "# Only train the head initially (linear probe)\n",
        "optimizer = torch.optim.AdamW(model.head.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "num_epochs_sup = 100\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs_sup):\n",
        "    print(f\"\\nSupervised Epoch {epoch+1}/{num_epochs_sup}\")\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, prefix=\"Val\")\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    print(f\"  Train loss: {train_loss:.4f}, acc: {train_acc:.4f}\")\n",
        "    print(f\"  Val   loss: {val_loss:.4f}, acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), supervised_ckpt_path)\n",
        "        print(f\"  --> New best model saved with val acc = {best_val_acc:.4f}\")\n",
        "\n",
        "# ---- Plots ----\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label=\"Train loss\")\n",
        "plt.plot(val_losses, label=\"Val loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Supervised training vs validation loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_accs, label=\"Train acc\")\n",
        "plt.plot(val_accs, label=\"Val acc\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Supervised training vs validation accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"Best val acc:\", best_val_acc)\n",
        "print(\"Best classifier saved at:\", supervised_ckpt_path)\n"
      ],
      "metadata": {
        "id": "quKVCtT6a0-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test evaluation (using best model)"
      ],
      "metadata": {
        "id": "mpXAogIra5Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "supervised_ckpt_path"
      ],
      "metadata": {
        "id": "lpz4jrMkz6At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dino_backbone_ckpt=\"/content/drive/MyDrive/FYP/foci_dino_backbone_70_70.pth\"\n",
        "supervised_ckpt_path=\"/content/drive/MyDrive/FYP/foci_dino_supervised_classifier.pth\""
      ],
      "metadata": {
        "id": "NXX9tucuz1-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "best_model = FociDINOClassifier(backbone_ckpt=dino_backbone_ckpt).to(device)\n",
        "best_model.load_state_dict(torch.load(supervised_ckpt_path, map_location=device))\n",
        "\n",
        "test_loss, test_acc = evaluate(best_model, test_loader, criterion, prefix=\"Test\")\n",
        "print(f\"\\nTEST RESULTS -> loss: {test_loss:.4f}, acc: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "AMaNiYiFa5s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference helpers + example usage"
      ],
      "metadata": {
        "id": "nhle1WOza8aG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # -------------------------------\n",
        "# # INFERENCE HELPERS\n",
        "# # -------------------------------\n",
        "# @torch.no_grad()\n",
        "# def infer_on_loader(model, loader, threshold=0.5):\n",
        "#     model.eval()\n",
        "#     all_probs = []\n",
        "#     all_preds = []\n",
        "#     for imgs, _ in tqdm(loader, desc=\"Infer\", leave=False):\n",
        "#         imgs = imgs.to(device, non_blocking=True)\n",
        "#         logits = model(imgs)\n",
        "#         probs = torch.sigmoid(logits)\n",
        "#         preds = (probs >= threshold).float()\n",
        "#         all_probs.extend(probs.cpu().tolist())\n",
        "#         all_preds.extend(preds.cpu().tolist())\n",
        "#     return all_probs, all_preds\n",
        "\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def infer_on_csv(model, csv_path, transform=eval_transform,\n",
        "#                  out_csv=None, threshold=0.5):\n",
        "#     df = pd.read_csv(csv_path)\n",
        "\n",
        "#     class InferenceDataset(Dataset):\n",
        "#         def __init__(self, df, transform):\n",
        "#             self.paths = df[\"image_path\"].tolist()\n",
        "#             self.transform = transform\n",
        "#         def __len__(self):\n",
        "#             return len(self.paths)\n",
        "#         def __getitem__(self, idx):\n",
        "#             img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "#             img = self.transform(img)\n",
        "#             return img, 0.0  # dummy label\n",
        "\n",
        "#     dataset = InferenceDataset(df, transform)\n",
        "#     loader = DataLoader(dataset, batch_size=16, shuffle=False,\n",
        "#                         num_workers=2, pin_memory=True)\n",
        "\n",
        "#     probs, preds = infer_on_loader(model, loader, threshold=threshold)\n",
        "#     df[\"prob_pos\"] = probs\n",
        "#     df[\"pred_label\"] = preds\n",
        "\n",
        "#     if out_csv is not None:\n",
        "#         df.to_csv(out_csv, index=False)\n",
        "#         print(\"Saved inference results to:\", out_csv)\n",
        "\n",
        "#     return df\n",
        "\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def infer_single_image(model, img_path, transform=eval_transform, threshold=0.5):\n",
        "#     img = Image.open(img_path).convert(\"RGB\")\n",
        "#     img = transform(img)\n",
        "#     img = img.unsqueeze(0).to(device)\n",
        "#     logit = model(img)\n",
        "#     prob = torch.sigmoid(logit).item()\n",
        "#     pred = 1.0 if prob >= threshold else 0.0\n",
        "#     return prob, pred\n",
        "\n",
        "\n",
        "# import os\n",
        "# import numpy as np\n",
        "# from PIL import Image\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def run_inference_and_visualize_csv(model,\n",
        "#                                     csv_path,\n",
        "#                                     transform=eval_transform,\n",
        "#                                     threshold=0.5,\n",
        "#                                     out_csv=None,\n",
        "#                                     max_rows=None):\n",
        "#     \"\"\"\n",
        "#     For each row in csv_path:\n",
        "#       - loads image_path\n",
        "#       - runs model -> prob, pred\n",
        "#       - computes gradient-based saliency map wrt input\n",
        "#       - displays [image | image + heatmap] with file path\n",
        "#     Also returns df with prob_pos & pred_label columns and optionally saves it.\n",
        "#     \"\"\"\n",
        "#     df = pd.read_csv(csv_path)\n",
        "#     print(\"Num rows in CSV:\", len(df))\n",
        "\n",
        "#     probs_all = []\n",
        "#     preds_all = []\n",
        "\n",
        "#     model.eval()\n",
        "\n",
        "#     # Optionally limit number of rows visualized\n",
        "#     if max_rows is None:\n",
        "#         max_rows = len(df)\n",
        "\n",
        "#     for idx, row in df.iloc[:max_rows].iterrows():\n",
        "#         img_path = row[\"image_path\"]\n",
        "#         true_label = row.get(\"label\", None)  # might be missing for unlabeled CSV\n",
        "\n",
        "#         # --- load + transform image ---\n",
        "#         img_pil = Image.open(img_path).convert(\"RGB\")\n",
        "#         x = transform(img_pil).unsqueeze(0).to(device)  # (1, 3, H, W)\n",
        "#         x.requires_grad_(True)\n",
        "\n",
        "#         # --- forward pass ---\n",
        "#         model.zero_grad()\n",
        "#         logit = model(x)            # (1,) from FociDINOClassifier\n",
        "#         prob = torch.sigmoid(logit)[0]\n",
        "#         pred = 1.0 if prob.item() >= threshold else 0.0\n",
        "\n",
        "#         # --- backward to get saliency wrt input ---\n",
        "#         prob.backward()\n",
        "#         grad = x.grad.detach()      # (1, 3, H, W)\n",
        "#         grad = grad.abs().max(dim=1)[0]  # (1, H, W) â†’ max over channels\n",
        "#         grad = grad[0].cpu().numpy()\n",
        "\n",
        "#         # normalize heatmap to [0, 1]\n",
        "#         g_min, g_max = grad.min(), grad.max()\n",
        "#         if g_max > g_min:\n",
        "#             grad_norm = (grad - g_min) / (g_max - g_min)\n",
        "#         else:\n",
        "#             grad_norm = np.zeros_like(grad)\n",
        "\n",
        "#         probs_all.append(prob.item())\n",
        "#         preds_all.append(pred)\n",
        "\n",
        "#         # --- PLOT: image + heatmap overlay ---\n",
        "#         fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "#         # left: original image\n",
        "#         axes[0].imshow(img_pil)\n",
        "#         axes[0].set_title(\"Original\", fontsize=10)\n",
        "#         axes[0].axis(\"off\")\n",
        "\n",
        "#         # right: overlay saliency\n",
        "#         axes[1].imshow(img_pil)\n",
        "#         axes[1].imshow(grad_norm, cmap=\"jet\", alpha=0.5)\n",
        "#         axes[1].set_title(\"Saliency / attention heatmap\", fontsize=10)\n",
        "#         axes[1].axis(\"off\")\n",
        "\n",
        "#         # big title with file path + labels\n",
        "#         title_lines = [f\"path: {img_path}\"]\n",
        "#         if true_label is not None:\n",
        "#             title_lines.append(f\"true={int(true_label)}, pred={int(pred)}, prob={prob.item():.3f}\")\n",
        "#         else:\n",
        "#             title_lines.append(f\"pred={int(pred)}, prob={prob.item():.3f}\")\n",
        "\n",
        "#         fig.suptitle(\"\\n\".join(title_lines), fontsize=8)\n",
        "#         plt.tight_layout()\n",
        "#         plt.show()\n",
        "\n",
        "#     # add predictions to df\n",
        "#     df = df.copy()\n",
        "#     df[\"prob_pos\"] = probs_all\n",
        "#     df[\"pred_label\"] = preds_all\n",
        "\n",
        "#     if out_csv is not None:\n",
        "#         df.to_csv(out_csv, index=False)\n",
        "#         print(\"Saved inference results to:\", out_csv)\n",
        "\n",
        "#     return df\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------------------\n",
        "# INFERENCE HELPERS\n",
        "# -------------------------------\n",
        "@torch.no_grad()\n",
        "def infer_on_loader(model, loader, threshold=0.5):\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_preds = []\n",
        "    for imgs, _ in tqdm(loader, desc=\"Infer\", leave=False):\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        logits = model(imgs)                      # shape: (B,) or (B,1)\n",
        "        logits = logits.view(-1)                  # ensure (B,)\n",
        "        probs = torch.sigmoid(logits)             # (B,)\n",
        "        preds = (probs >= threshold).float()      # (B,)\n",
        "        all_probs.extend(probs.cpu().tolist())\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "    return all_probs, all_preds\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer_on_csv(model, csv_path, transform,\n",
        "                 out_csv=None, threshold=0.5, batch_size=16):\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    class InferenceDataset(Dataset):\n",
        "        def __init__(self, df_, transform_):\n",
        "            self.paths = df_[\"image_path\"].tolist()\n",
        "            self.transform = transform_\n",
        "        def __len__(self):\n",
        "            return len(self.paths)\n",
        "        def __getitem__(self, idx):\n",
        "            img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "            img = self.transform(img)\n",
        "            return img, 0.0  # dummy label\n",
        "\n",
        "    dataset = InferenceDataset(df, transform)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False,\n",
        "                        num_workers=2, pin_memory=True)\n",
        "\n",
        "    probs, preds = infer_on_loader(model, loader, threshold=threshold)\n",
        "    df = df.copy()\n",
        "    df[\"prob_pos\"] = probs\n",
        "    df[\"pred_label\"] = preds\n",
        "\n",
        "    if out_csv is not None:\n",
        "        df.to_csv(out_csv, index=False)\n",
        "        print(\"Saved inference results to:\", out_csv)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer_single_image(model, img_path, transform, threshold=0.5):\n",
        "    model.eval()\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    x = transform(img).unsqueeze(0).to(device)\n",
        "    logit = model(x).view(-1)[0]\n",
        "    prob = torch.sigmoid(logit).item()\n",
        "    pred = 1.0 if prob >= threshold else 0.0\n",
        "    return prob, pred\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# SALIENCY + CSV VISUALIZATION (no checkerboard/grid artifacts)\n",
        "# -------------------------------\n",
        "def run_inference_and_visualize_csv(model,\n",
        "                                    csv_path,\n",
        "                                    transform,\n",
        "                                    threshold=0.5,\n",
        "                                    out_csv=None,\n",
        "                                    max_rows=None,\n",
        "                                    display_size=None):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(\"Num rows in CSV:\", len(df))\n",
        "\n",
        "    probs_all = []\n",
        "    preds_all = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if max_rows is None:\n",
        "        max_rows = len(df)\n",
        "\n",
        "    if display_size is None:\n",
        "        display_size = (IMAGE_SIZE, IMAGE_SIZE)\n",
        "\n",
        "    for idx, row in df.iloc[:max_rows].iterrows():\n",
        "        img_path = row[\"image_path\"]\n",
        "        true_label = row.get(\"label\", None)\n",
        "\n",
        "        img_pil = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        x = transform(img_pil).unsqueeze(0).to(device)  # (1,3,H,W)\n",
        "        x.requires_grad_(True)\n",
        "\n",
        "        model.zero_grad(set_to_none=True)\n",
        "\n",
        "        logit = model(x).view(-1)[0]          # scalar\n",
        "        prob = torch.sigmoid(logit)           # scalar in (0,1)\n",
        "        pred = 1.0 if prob.item() >= threshold else 0.0\n",
        "\n",
        "        prob.backward()\n",
        "\n",
        "        grad = x.grad.detach()                # (1,3,H,W)\n",
        "        grad = grad.abs().amax(dim=1)[0]      # (H,W) max over channels\n",
        "\n",
        "        # normalize to [0,1]\n",
        "        g_min, g_max = grad.min(), grad.max()\n",
        "        if (g_max - g_min) > 1e-12:\n",
        "            grad_norm = (grad - g_min) / (g_max - g_min)\n",
        "        else:\n",
        "            grad_norm = torch.zeros_like(grad)\n",
        "\n",
        "        grad_norm = grad_norm.cpu().numpy()   # (H,W)\n",
        "\n",
        "        probs_all.append(prob.item())\n",
        "        preds_all.append(pred)\n",
        "\n",
        "        img_disp = img_pil.resize(display_size, resample=Image.BILINEAR)\n",
        "\n",
        "        # --- plot ---\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(8, 4), constrained_layout=True)\n",
        "\n",
        "        # left: original (resized to match heatmap)\n",
        "        axes[0].imshow(img_disp, interpolation=\"bilinear\")\n",
        "        axes[0].set_title(\"Original\", fontsize=10)\n",
        "        axes[0].set_axis_off()\n",
        "\n",
        "        # right: overlay (smooth interpolation prevents checkerboard)\n",
        "        axes[1].imshow(img_disp, interpolation=\"bilinear\")\n",
        "        axes[1].imshow(\n",
        "            grad_norm,\n",
        "            cmap=\"jet\",\n",
        "            alpha=0.45,\n",
        "            interpolation=\"bilinear\",\n",
        "            vmin=0.0, vmax=1.0\n",
        "        )\n",
        "        axes[1].set_title(\"Saliency heatmap\", fontsize=10)\n",
        "        axes[1].set_axis_off()\n",
        "\n",
        "        title_lines = [f\"path: {img_path}\"]\n",
        "        if true_label is not None and not pd.isna(true_label):\n",
        "            title_lines.append(f\"true={int(true_label)}, pred={int(pred)}, prob={prob.item():.3f}\")\n",
        "        else:\n",
        "            title_lines.append(f\"pred={int(pred)}, prob={prob.item():.3f}\")\n",
        "\n",
        "        fig.suptitle(\"\\n\".join(title_lines), fontsize=8)\n",
        "        plt.show()\n",
        "\n",
        "        x.grad = None\n",
        "\n",
        "    df_out = df.iloc[:max_rows].copy()\n",
        "    df_out[\"prob_pos\"] = probs_all\n",
        "    df_out[\"pred_label\"] = preds_all\n",
        "\n",
        "    if out_csv is not None:\n",
        "        df_out.to_csv(out_csv, index=False)\n",
        "        print(\"Saved inference results to:\", out_csv)\n",
        "\n",
        "    return df_out"
      ],
      "metadata": {
        "id": "ym5MDdp6a8t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter\n",
        "import torch\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "def pad_to_square(pil_img, fill=0):\n",
        "    \"\"\"Return padded square PIL + (pad_left, pad_top, new_side).\"\"\"\n",
        "    w, h = pil_img.size\n",
        "    side = max(w, h)\n",
        "    pad_left = (side - w) // 2\n",
        "    pad_top  = (side - h) // 2\n",
        "    pad_right = side - w - pad_left\n",
        "    pad_bottom = side - h - pad_top\n",
        "    padded = TF.pad(pil_img, padding=[pad_left, pad_top, pad_right, pad_bottom], fill=fill)\n",
        "    return padded, pad_left, pad_top, side\n",
        "\n",
        "def unpad_map(map_sq, pad_left, pad_top, orig_w, orig_h, side):\n",
        "    \"\"\"\n",
        "    map_sq: saliency map on square (side x side) in numpy\n",
        "    Crops out padding region to get back to original (orig_h x orig_w).\n",
        "    \"\"\"\n",
        "    x0 = pad_left\n",
        "    y0 = pad_top\n",
        "    x1 = x0 + orig_w\n",
        "    y1 = y0 + orig_h\n",
        "    return map_sq[y0:y1, x0:x1]\n",
        "\n",
        "def run_inference_and_visualize_csv_whole_image(\n",
        "    model,\n",
        "    csv_path,\n",
        "    threshold=0.5,\n",
        "    out_csv=None,\n",
        "    max_rows=None,\n",
        "    blur_sigma=5,\n",
        "    output_dir=None,\n",
        "    save_fig=True,\n",
        "    show_fig=True,\n",
        "    dpi=150,\n",
        "    input_size=224,\n",
        "):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(\"Num rows in CSV:\", len(df))\n",
        "\n",
        "    probs_all, preds_all, kept_indices = [], [], []\n",
        "    model.eval()\n",
        "\n",
        "    if max_rows is None:\n",
        "        max_rows = len(df)\n",
        "\n",
        "    if output_dir is not None:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(\"Saving figures to:\", output_dir)\n",
        "\n",
        "    for idx, row in df.iloc[:max_rows].iterrows():\n",
        "        img_path = row[\"image_path\"]\n",
        "        true_label = row.get(\"label\", None)\n",
        "\n",
        "        try:\n",
        "            img_pil = Image.open(img_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            print(f\"[SKIP] Error loading {img_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        orig_w, orig_h = img_pil.size\n",
        "\n",
        "        img_sq, pad_left, pad_top, side = pad_to_square(img_pil, fill=0)\n",
        "        img_in = img_sq.resize((input_size, input_size), resample=Image.BILINEAR)\n",
        "\n",
        "        x = TF.to_tensor(img_in)  # (3,H,W) float [0,1]\n",
        "        x = TF.normalize(x, mean=[0.5,0.5,0.5], std=[0.25,0.25,0.25])\n",
        "        x = x.unsqueeze(0).to(device)\n",
        "        x.requires_grad_(True)\n",
        "\n",
        "        model.zero_grad(set_to_none=True)\n",
        "\n",
        "        logit = model(x).view(-1)[0]\n",
        "        prob = torch.sigmoid(logit)\n",
        "        prob_val = float(prob.item())\n",
        "        pred_val = 1.0 if prob_val >= threshold else 0.0\n",
        "\n",
        "        prob.backward()\n",
        "\n",
        "        grad = x.grad.detach()                 # (1,3,224,224)\n",
        "        grad = grad.abs().amax(dim=1)[0]       # (224,224)\n",
        "        grad = grad.cpu().numpy()\n",
        "\n",
        "        grad = gaussian_filter(grad, sigma=blur_sigma)\n",
        "\n",
        "        g_min, g_max = grad.min(), grad.max()\n",
        "        grad_norm = (grad - g_min) / (g_max - g_min + 1e-12)\n",
        "\n",
        "        kept_indices.append(idx)\n",
        "        probs_all.append(prob_val)\n",
        "        preds_all.append(pred_val)\n",
        "\n",
        "        map_sq = Image.fromarray((grad_norm * 255).astype(np.uint8)).resize(\n",
        "            (side, side), resample=Image.BILINEAR\n",
        "        )\n",
        "        map_sq = np.array(map_sq).astype(np.float32) / 255.0\n",
        "\n",
        "        map_unpadded = unpad_map(map_sq, pad_left, pad_top, orig_w, orig_h, side)\n",
        "\n",
        "        map_final = Image.fromarray((map_unpadded * 255).astype(np.uint8)).resize(\n",
        "            (orig_w, orig_h), resample=Image.BILINEAR\n",
        "        )\n",
        "        map_final = np.array(map_final).astype(np.float32) / 255.0\n",
        "\n",
        "        assert map_final.shape == (orig_h, orig_w), f\"Mismatch map {map_final.shape} vs img {(orig_h, orig_w)}\"\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True)\n",
        "\n",
        "        axes[0].imshow(img_pil)\n",
        "        axes[0].set_title(\"Original (full image)\", fontsize=10)\n",
        "        axes[0].axis(\"off\")\n",
        "\n",
        "        axes[1].imshow(map_final, cmap=\"jet\", vmin=0.0, vmax=1.0)\n",
        "        axes[1].set_title(f\"Saliency (full img)\", fontsize=10)\n",
        "        axes[1].axis(\"off\")\n",
        "\n",
        "        axes[2].imshow(img_pil)\n",
        "        axes[2].imshow(map_final, cmap=\"jet\", alpha=0.45, vmin=0.0, vmax=1.0)\n",
        "        axes[2].set_title(\"Overlay (aligned to full image)\", fontsize=10)\n",
        "        axes[2].axis(\"off\")\n",
        "\n",
        "        title_lines = [f\"path: {os.path.basename(img_path)}\"]\n",
        "        if true_label is not None and not pd.isna(true_label):\n",
        "            title_lines.append(f\"true={int(true_label)}, pred={int(pred_val)}, prob={prob_val:.3f}\")\n",
        "        else:\n",
        "            title_lines.append(f\"pred={int(pred_val)}, prob={prob_val:.3f}\")\n",
        "        fig.suptitle(\"\\n\".join(title_lines), fontsize=9)\n",
        "\n",
        "        if save_fig and output_dir is not None:\n",
        "            base = os.path.splitext(os.path.basename(img_path))[0]\n",
        "            save_name = f\"{idx:05d}_{base}_pred{int(pred_val)}_p{prob_val:.3f}.png\"\n",
        "            fig.savefig(os.path.join(output_dir, save_name), dpi=dpi, bbox_inches=\"tight\")\n",
        "\n",
        "        if show_fig:\n",
        "            plt.show()\n",
        "        plt.close(fig)\n",
        "\n",
        "        x.grad = None\n",
        "\n",
        "    df_out = df.loc[kept_indices].copy()\n",
        "    df_out[\"prob_pos\"] = probs_all\n",
        "    df_out[\"pred_label\"] = preds_all\n",
        "\n",
        "    if out_csv is not None:\n",
        "        df_out.to_csv(out_csv, index=False)\n",
        "        print(\"Saved inference results to:\", out_csv)\n",
        "\n",
        "    return df_out\n"
      ],
      "metadata": {
        "id": "z3F62NPNgZMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "supervised_ckpt_path"
      ],
      "metadata": {
        "id": "hiy8fO2rnHWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dino_backbone_ckpt=\"/content/drive/MyDrive/FYP/foci_dino_backbone_ALLimgs_v02.pth\"\n",
        "supervised_ckpt_path=\"/content/drive/MyDrive/FYP/foci_dino_supervised_classifier_v02.pth\""
      ],
      "metadata": {
        "id": "Gx17WgpgiAk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = FociDINOClassifier(backbone_ckpt=dino_backbone_ckpt).to(device)\n",
        "best_model.load_state_dict(torch.load(supervised_ckpt_path, map_location=device))\n",
        "\n",
        "df_pred = run_inference_and_visualize_csv_whole_image(\n",
        "    best_model,\n",
        "    csv_path=\"/content/drive/MyDrive/FYP/foci_supervised_test_v02.csv\",\n",
        "    threshold=0.5,\n",
        "    out_csv=\"/content/drive/MyDrive/FYP/foci_supervised_test_v02_pred.csv\",\n",
        "    max_rows=None,\n",
        "    blur_sigma=5,\n",
        "    output_dir=\"/content/drive/MyDrive/FYP/saliency_panels_fullimage_v02\",\n",
        "    save_fig=True,\n",
        "    show_fig=True,\n",
        "    input_size=224\n",
        ")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KClgUVW5gd3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dino_backbone_ckpt=\"/content/drive/MyDrive/FYP/foci_dino_backbone_ALLimgs_v03.pth\"\n",
        "supervised_ckpt_path=\"/content/drive/MyDrive/FYP/foci_dino_supervised_classifier_v03.pth\"\n",
        "\n",
        "dino_backbone_ckpt=\"/content/drive/MyDrive/FYP/foci_dino_backbone_70_70.pth\"\n",
        "supervised_ckpt_path=\"/content/drive/MyDrive/FYP/foci_dino_supervised_classifier.pth\""
      ],
      "metadata": {
        "id": "BBFDqVKhbExP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob, pred = infer_single_image(\n",
        "    best_model,\n",
        "    \"/content/drive/MyDrive/FYP/groundtruth/4Gy_gH2AX_4Hr/cell_0207.png\",\n",
        "    threshold=0.5\n",
        ")\n",
        "print(\"Prob(class=1):\", prob, \"Predicted label:\", pred)\n"
      ],
      "metadata": {
        "id": "ZGPQmEQdbJIa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}